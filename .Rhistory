gplot(nox_london) +
geom_tile(aes(fill = value))
#read library exactextractr:
library(exactextractr)
#Calculate average PM10 pollution by area using exact_extract
london_income$pm10 <- exact_extract(pm10_london, london_income, fun = 'mean', progress = F)
#Calculate average NOx pollution by area using exact_extract
london_income$nox <- exact_extract(nox_london, london_income, fun = 'mean', progress = F)
#Plot average PM10 pollution
ggplot() +
geom_sf(data = london_income, color=NA, alpha = 1, aes(fill = pm10)) +
xlab("Longitude") + ylab("Latitude") +
scale_fill_viridis_c(name = "PM10 mu g /m3",
guide = guide_colorbar(title.position = "top",
barwidth = 10,
barheight = 0.5)) +
theme(legend.position = "bottom", legend.direction = 'horizontal') +
ggtitle("Average annual PM10 pollution") +
annotation_scale(location = "br", width_hint = 0.3)
#Plot average NOx pollution
ggplot() +
geom_sf(data = london_income, color=NA, alpha = 1, aes(fill = nox)) +
xlab("Longitude") + ylab("Latitude") +
scale_fill_viridis_c(name = "NOx mu g/m3",
guide = guide_colorbar(title.position = "top",
barwidth = 10,
barheight = 0.5)) +
theme(legend.position = "bottom", legend.direction = 'horizontal') +
ggtitle("Average annual NOx pollution") +
annotation_scale(location = "br", width_hint = 0.3)
ggplot(london_income, aes(x= nox, y = housing_costs)) +
geom_point()+
labs(y= "Costs of housing", x="PM10")+
geom_smooth(method=lm)+
stat_cor(method = "pearson", label.y = 16000, label.x = 30)
summary(lm(housing_costs ~ I(dist_to_center/1000) + nox + pm10 + canopy_density , data = london_income))
#Regression of has_monitor on income after housing, housing costs, dist_to_center, nox and pm10 levels:
summary(lm(has_monitor~ Inc_AH + housing_costs + dist_to_center + nox + pm10,   data = london_income ))
setwd(choose.dir())
library(sf)
# read data on London MSOA:
london_msoa <- read_sf('Data/London_boundaries/MSOA_2011_London_gen_MHW.shp')
#Just type the name of the previously defined object and run this chunk:
london_msoa
library(ggplot2)
library(ggspatial)
#Create a plot of the data using ggplot(), geom_sf(),
# add axis labels using xlab() and ylab()
# add a scale bar in the bottom right corner using annotation_scale()
ggplot() +
geom_sf(data = london_msoa) +
xlab("Longitude") + ylab("Latitude") +
annotation_scale(location = "br", width_hint = 0.3)
#Replicate the previous plot but adjust the graticule. To do this, overwrite the default datum argument in the geom_sf() function by passing a CRS object to datum  (hint: you can obtain the CRS of an sf object using st_crs())
ggplot() +
geom_sf(data = london_msoa) +
xlab("Easting (in metres)") + ylab("Northing (in metres)") +
annotation_scale(location = "br", width_hint = 0.3) +
coord_sf(datum = st_crs(london_msoa))
# Read the file on income before housing costs using the read.csv() command:
inc_before_housing <- read.csv('Data/UK_income/netannualincomebeforehousingcosts2018.csv', skip = 4)
# Do the same for the income after housing costs:
inc_after_housing <- read.csv('Data/UK_income/netannualincomeafterhousingcosts2018.csv', skip = 4)
# And have a look at one of the files:
head(inc_before_housing)
# As some columns have inconvenient names, rename them using the colnames() command:
colnames(inc_before_housing)[7:10] <- c('Inc_BH', 'Inc_BH_CI_L',
'Inc_BH_CI_H', 'Inc_BH_CI_range')
colnames(inc_after_housing)[7:10] <- c('Inc_AH', 'Inc_AH_CI_L',
'Inc_AH_CI_H', 'Inc_AH_CI_range')
#Also change the format of these columns - this is a bit tricky here. We have to remove the "," signs using the gsub() function and transform the variables to numeric format using as.numeric:
## You can either do this using a for loop:
for (col in c('Inc_BH', 'Inc_BH_CI_L','Inc_BH_CI_H', 'Inc_BH_CI_range')){
inc_before_housing[, col] <- as.numeric(gsub(",", "", inc_before_housing[, col]))
}
## Or achieve the same using the sapply command:
inc_after_housing[, c('Inc_AH', 'Inc_AH_CI_L','Inc_AH_CI_H', 'Inc_AH_CI_range')] <-
sapply(inc_after_housing[, c('Inc_AH', 'Inc_AH_CI_L','Inc_AH_CI_H', 'Inc_AH_CI_range')],
function(x) as.numeric(gsub(',', '', x)))
# Generate a new data set called london_income by merging the spatial London MSOA data and the data set on income before housing costs and income after housing costs.
# When doing the merge, we only want to take the Inc_BH and Inc_AH columns of the income data, so make sure to only select these columns:
london_income <- merge(london_msoa,
inc_before_housing[, c("MSOA.code", "Inc_BH")],
by.x = "MSOA11CD", by.y = "MSOA.code")
#Merge resulting data with the income data after housing cost adjustments:
london_income <- merge(london_income,
inc_after_housing[, c('MSOA.code', 'Inc_AH')],
by.x = "MSOA11CD", by.y = "MSOA.code")
#Generate a plot of the spatial distribution of income using ggplot(). Add aes(fill = Inc_BH) to the geom_sf() call to color spatial polygons according to the average income before housing costs. Feel free to add additional ggplot options to make the plot look nice.
ggplot() +
geom_sf(data = london_income, aes(fill = Inc_BH), color = NA) +
xlab("Longitude") + ylab("Latitude") +
annotation_scale(location = "br", width_hint = 0.3) +
ggtitle("Average annual net income before housing costs") +
scale_fill_viridis_c(option = "inferno", name = 'Net Annual Income [GBP]',
breaks = c(0, 15000, 30000, 45000, 60000),
labels = c('0', '15K', '30K', '45K', '60K'),
limits = c(18000, 67100),
guide = guide_colorbar(title.position = "top",
barwidth = 10,
barheight = 0.5)) +
theme(legend.position = "bottom", legend.direction = 'horizontal')
ggplot() +
geom_sf(data = london_income, aes(fill = Inc_AH), color = NA) +
xlab("Longitude") + ylab("Latitude") +
scale_fill_viridis_c(option = "inferno", name = 'Net Annual Income [GBP]',
breaks = c(0, 15000, 30000, 45000, 60000),
labels = c('0', '15K', '30K', '45K', '60K'),
limits = c(18000, 67100),
guide = guide_colorbar(title.position = "top",
barwidth = 10,
barheight = 0.5)) +
theme(legend.position = "bottom", legend.direction = 'horizontal') +
ggtitle("Average annual net income after housing costs") +
annotation_scale(location = "br", width_hint = 0.3)
#Generate a new housing cost variable in the london_income data called housing_costs:
london_income$housing_costs <- london_income$Inc_BH - london_income$Inc_AH
ggplot() +
geom_sf(data = london_income, aes(fill = housing_costs), color = NA) +
xlab("Longitude") + ylab("Latitude") +
scale_fill_viridis_c(option = "inferno", name = 'Average housing costs [GBP]',
breaks = seq(0, 25000, 5000),
labels = seq(0, 25000, 5000),
guide = guide_colorbar(title.position = "top",
barwidth = 10,
barheight = 0.5)) +
theme(legend.position = "bottom", legend.direction = 'horizontal') +
ggtitle("Average annual housing costs") +
annotation_scale(location = "br", width_hint = 0.3)
library(mapview)
# Use the mapview function to make an interactive plot of income in London. Specify the column to be visualized using zcol:
mapview(london_income, zcol = 'housing_costs')
# Subset London income data to City of London:
city_of_london <- london_income[london_income$MSOA11CD == "E02000001",]
# Find the centroid of the City of London using st_centroid()
city_of_london_center <- st_centroid(city_of_london)
#Find the centroid of all other MSOA polygons:
london_income_center <- st_centroid(london_income)
#Calculate the distance between the city center and all other MSOA centroids using st_distance and add this variable to the data set london_income. To make sure the output from the st_distance function is a vector, add the as.vector() command:
london_income$dist_to_center <- as.vector(st_distance(london_income_center, city_of_london_center))
ggplot() +
geom_sf(data = london_income, aes(fill = dist_to_center),
color = NA) +
xlab("Longitude") + ylab("Latitude") +
scale_fill_viridis_c(option = "inferno")
library(ggpubr)
ggplot(london_income, aes(x = dist_to_center, y = housing_costs)) +
geom_point( alpha = 0.5) +
labs(x= "Distance to city center [in metres]", y="Annual housing costs [GBP]")+
geom_smooth(method = 'lm')+
stat_cor(method = "pearson", label.x = 15000, label.y = 20000)
# Specify the regression formula:
lm1 <- lm(housing_costs ~ dist_to_center, data = london_income)
# Display the summary of results:
summary(lm1)
# Load data:
canopy_lsoa <- read_sf('Data/London_trees/gla-canopy-lsoas.shp')
# Explore data
head(canopy_lsoa)
# Transform the canopy_lsoa file into the same projection as the london_income file using the function st_transform:
canopy_lsoa <- st_transform(canopy_lsoa, crs = st_crs(london_income))
# Subset data to canopy_per:
canopy_lsoa<- canopy_lsoa[,"canopy_per"]
# Convert canopy_per to numeric:
canopy_lsoa$canopy_per <- as.numeric(canopy_lsoa$canopy_per)
ggplot() +
geom_sf(data = canopy_lsoa,color=NA, alpha = 1, aes(fill = canopy_per)) +
xlab("Longitude") + ylab("Latitude") +
scale_fill_viridis_c(name = 'Cover density',
guide = guide_colorbar(title.position = "top",
barwidth = 10,
barheight = 0.5)) +
theme(legend.position = "bottom", legend.direction = 'horizontal') +
ggtitle("Canopy cover density") +
annotation_scale(location = "br", width_hint = 0.3)
# use the aggregate function to bring the canopy_lsoa to the right spatial extent:
canopy_msoa <- aggregate(x = canopy_lsoa, by = london_income,
FUN = mean, areaWeighted = TRUE)
#Check the resulting output e.g. using head() or str()
head(canopy_msoa)
# Add data on canopy density to london income data
london_income$canopy_density <- canopy_msoa$canopy_per
ggplot() +
geom_sf(data = london_income, color=NA, alpha = 1, aes(fill = canopy_density)) +
xlab("Longitude") + ylab("Latitude") +
scale_fill_viridis_c( name = 'Cover density [percent]',
guide = guide_colorbar(title.position = "top",
barwidth = 10,
barheight = 0.5)) +
theme(legend.position = "bottom", legend.direction = 'horizontal') +
ggtitle("Canopy cover density") +
annotation_scale(location = "br", width_hint = 0.3)
# Add a scatter plot of income and canopy cover. Also add a regression line to the plot using geom_smooth() and a correlation coefficient using stat_cor():
# Once for income before housing
ggplot(london_income, aes(x = Inc_BH, y= canopy_density)) +
geom_point(alpha = 0.5) +
labs(x= "Income before housing costs", y="Canopy density")+
geom_smooth(method=lm)+
stat_cor(method = "pearson", label.x = 20000, label.y = 40)
# And once for income after housing
ggplot(london_income, aes(x = Inc_AH, y= canopy_density)) +
geom_point(alpha = 0.5) +
labs(x= "Income after housing costs", y="Canopy density")+
geom_smooth(method=lm)+
stat_cor(method = "pearson", label.x = 20000, label.y = 40)
ggplot(london_income, aes(x= canopy_density, y = housing_costs)) +
geom_point()+
labs(y= "Costs of housing", x="Canopy density")+
geom_smooth(method=lm)+
stat_cor(method = "pearson", label.y = 16000, label.x = 30)
library(raster)
pm10_london <- raster("Data/London_air/LAEI2016_2016_PM10.asc")
nox_london <- raster("Data/London_air/LAEI2016_2016_NOX.asc")
mapview(pm10_london)
library(rasterVis)
gplot(nox_london) +
geom_tile(aes(fill = value))
#read library exactextractr:
library(exactextractr)
#Calculate average PM10 pollution by area using exact_extract
london_income$pm10 <- exact_extract(pm10_london, london_income, fun = 'mean', progress = F)
#Calculate average NOx pollution by area using exact_extract
london_income$nox <- exact_extract(nox_london, london_income, fun = 'mean', progress = F)
#Plot average PM10 pollution
ggplot() +
geom_sf(data = london_income, color=NA, alpha = 1, aes(fill = pm10)) +
xlab("Longitude") + ylab("Latitude") +
scale_fill_viridis_c(name = "PM10 mu g /m3",
guide = guide_colorbar(title.position = "top",
barwidth = 10,
barheight = 0.5)) +
theme(legend.position = "bottom", legend.direction = 'horizontal') +
ggtitle("Average annual PM10 pollution") +
annotation_scale(location = "br", width_hint = 0.3)
#Plot average NOx pollution
ggplot() +
geom_sf(data = london_income, color=NA, alpha = 1, aes(fill = nox)) +
xlab("Longitude") + ylab("Latitude") +
scale_fill_viridis_c(name = "NOx mu g/m3",
guide = guide_colorbar(title.position = "top",
barwidth = 10,
barheight = 0.5)) +
theme(legend.position = "bottom", legend.direction = 'horizontal') +
ggtitle("Average annual NOx pollution") +
annotation_scale(location = "br", width_hint = 0.3)
ggplot(london_income, aes(x= nox, y = housing_costs)) +
geom_point()+
labs(y= "Costs of housing", x="PM10")+
geom_smooth(method=lm)+
stat_cor(method = "pearson", label.y = 16000, label.x = 30)
summary(lm(housing_costs ~ I(dist_to_center/1000) + nox + pm10 + canopy_density , data = london_income))
setwd(choose.dir())
library(sf)
# read data on London MSOA:
london_msoa <- read_sf('Data/London_boundaries/MSOA_2011_London_gen_MHW.shp')
#Just type the name of the previously defined object and run this chunk:
london_msoa
library(ggplot2)
library(ggspatial)
#Create a plot of the data using ggplot(), geom_sf(),
# add axis labels using xlab() and ylab()
# add a scale bar in the bottom right corner using annotation_scale()
ggplot() +
geom_sf(data = london_msoa) +
xlab("Longitude") + ylab("Latitude") +
annotation_scale(location = "br", width_hint = 0.3)
#Replicate the previous plot but adjust the graticule. To do this, overwrite the default datum argument in the geom_sf() function by passing a CRS object to datum  (hint: you can obtain the CRS of an sf object using st_crs())
ggplot() +
geom_sf(data = london_msoa) +
xlab("Easting (in metres)") + ylab("Northing (in metres)") +
annotation_scale(location = "br", width_hint = 0.3) +
coord_sf(datum = st_crs(london_msoa))
# Read the file on income before housing costs using the read.csv() command:
inc_before_housing <- read.csv('Data/UK_income/netannualincomebeforehousingcosts2018.csv', skip = 4)
# Do the same for the income after housing costs:
inc_after_housing <- read.csv('Data/UK_income/netannualincomeafterhousingcosts2018.csv', skip = 4)
# And have a look at one of the files:
head(inc_before_housing)
# As some columns have inconvenient names, rename them using the colnames() command:
colnames(inc_before_housing)[7:10] <- c('Inc_BH', 'Inc_BH_CI_L',
'Inc_BH_CI_H', 'Inc_BH_CI_range')
colnames(inc_after_housing)[7:10] <- c('Inc_AH', 'Inc_AH_CI_L',
'Inc_AH_CI_H', 'Inc_AH_CI_range')
#Also change the format of these columns - this is a bit tricky here. We have to remove the "," signs using the gsub() function and transform the variables to numeric format using as.numeric:
## You can either do this using a for loop:
for (col in c('Inc_BH', 'Inc_BH_CI_L','Inc_BH_CI_H', 'Inc_BH_CI_range')){
inc_before_housing[, col] <- as.numeric(gsub(",", "", inc_before_housing[, col]))
}
## Or achieve the same using the sapply command:
inc_after_housing[, c('Inc_AH', 'Inc_AH_CI_L','Inc_AH_CI_H', 'Inc_AH_CI_range')] <-
sapply(inc_after_housing[, c('Inc_AH', 'Inc_AH_CI_L','Inc_AH_CI_H', 'Inc_AH_CI_range')],
function(x) as.numeric(gsub(',', '', x)))
# Generate a new data set called london_income by merging the spatial London MSOA data and the data set on income before housing costs and income after housing costs.
# When doing the merge, we only want to take the Inc_BH and Inc_AH columns of the income data, so make sure to only select these columns:
london_income <- merge(london_msoa,
inc_before_housing[, c("MSOA.code", "Inc_BH")],
by.x = "MSOA11CD", by.y = "MSOA.code")
#Merge resulting data with the income data after housing cost adjustments:
london_income <- merge(london_income,
inc_after_housing[, c('MSOA.code', 'Inc_AH')],
by.x = "MSOA11CD", by.y = "MSOA.code")
#Generate a plot of the spatial distribution of income using ggplot(). Add aes(fill = Inc_BH) to the geom_sf() call to color spatial polygons according to the average income before housing costs. Feel free to add additional ggplot options to make the plot look nice.
ggplot() +
geom_sf(data = london_income, aes(fill = Inc_BH), color = NA) +
xlab("Longitude") + ylab("Latitude") +
annotation_scale(location = "br", width_hint = 0.3) +
ggtitle("Average annual net income before housing costs") +
scale_fill_viridis_c(option = "inferno", name = 'Net Annual Income [GBP]',
breaks = c(0, 15000, 30000, 45000, 60000),
labels = c('0', '15K', '30K', '45K', '60K'),
limits = c(18000, 67100),
guide = guide_colorbar(title.position = "top",
barwidth = 10,
barheight = 0.5)) +
theme(legend.position = "bottom", legend.direction = 'horizontal')
ggplot() +
geom_sf(data = london_income, aes(fill = Inc_AH), color = NA) +
xlab("Longitude") + ylab("Latitude") +
scale_fill_viridis_c(option = "inferno", name = 'Net Annual Income [GBP]',
breaks = c(0, 15000, 30000, 45000, 60000),
labels = c('0', '15K', '30K', '45K', '60K'),
limits = c(18000, 67100),
guide = guide_colorbar(title.position = "top",
barwidth = 10,
barheight = 0.5)) +
theme(legend.position = "bottom", legend.direction = 'horizontal') +
ggtitle("Average annual net income after housing costs") +
annotation_scale(location = "br", width_hint = 0.3)
#Generate a new housing cost variable in the london_income data called housing_costs:
london_income$housing_costs <- london_income$Inc_BH - london_income$Inc_AH
ggplot() +
geom_sf(data = london_income, aes(fill = housing_costs), color = NA) +
xlab("Longitude") + ylab("Latitude") +
scale_fill_viridis_c(option = "inferno", name = 'Average housing costs [GBP]',
breaks = seq(0, 25000, 5000),
labels = seq(0, 25000, 5000),
guide = guide_colorbar(title.position = "top",
barwidth = 10,
barheight = 0.5)) +
theme(legend.position = "bottom", legend.direction = 'horizontal') +
ggtitle("Average annual housing costs") +
annotation_scale(location = "br", width_hint = 0.3)
library(mapview)
# Use the mapview function to make an interactive plot of income in London. Specify the column to be visualized using zcol:
mapview(london_income, zcol = 'housing_costs')
# Subset London income data to City of London:
city_of_london <- london_income[london_income$MSOA11CD == "E02000001",]
# Find the centroid of the City of London using st_centroid()
city_of_london_center <- st_centroid(city_of_london)
#Find the centroid of all other MSOA polygons:
london_income_center <- st_centroid(london_income)
#Calculate the distance between the city center and all other MSOA centroids using st_distance and add this variable to the data set london_income. To make sure the output from the st_distance function is a vector, add the as.vector() command:
london_income$dist_to_center <- as.vector(st_distance(london_income_center, city_of_london_center))
ggplot() +
geom_sf(data = london_income, aes(fill = dist_to_center),
color = NA) +
xlab("Longitude") + ylab("Latitude") +
scale_fill_viridis_c(option = "inferno")
library(ggpubr)
ggplot(london_income, aes(x = dist_to_center, y = housing_costs)) +
geom_point( alpha = 0.5) +
labs(x= "Distance to city center [in metres]", y="Annual housing costs [GBP]")+
geom_smooth(method = 'lm')+
stat_cor(method = "pearson", label.x = 15000, label.y = 20000)
# Specify the regression formula:
lm1 <- lm(housing_costs ~ dist_to_center, data = london_income)
# Display the summary of results:
summary(lm1)
library(sf) # processing of spatial data in R
london_msoa <- read_sf('C:/Users/Rob/Documents/ArcGIS/tin.lyr')
tin <- read_sf('C:/Users/Rob/Documents/ArcGIS/tin.lyr')
st_drivers()
#current_dir <- "./IO_partII"
current_dir <- "G:/Mi unidad/Columbia_doc/My Classes/IO I/Assignment/final_version/IO_partII/"
setwd(current_dir)
outputs_dir <- paste0(current_dir,"Outputs")
bellman_data_file <- paste0(current_dir,"Data/data_for_bellman_computations.csv")
panel_data_file <- paste0(current_dir,"Data/analysis_data.csv")
source("Bellman_functions.R")
# Read Bellman data
data_bellman <- read.csv(bellman_data_file)
data <- subset(data_bellman,naics_recode == 1)
data$index <- 1:nrow(data)
# Read analysis data for lilkelihood computations
panel_data <- read.csv(panel_data_file)
panel_data <- subset(panel_data,naics_recode == 1)
# Define the working parameters
params <- list()
params$data <- data
params$panel_data <- panel_data
params$N1 <- 161  # number of states per each omega1
params$n_omega1 <- length(unique((data$omega1))) # number of omega1 states
params$DAVgrid <- (0:19)/19*9.5  # DAV grid
params$n_DAV <- length(params$DAVgrid) # number of points in DAV grid
params$DAV_deprate <- 0.1  # DAV depreciation rate
params$beta <- 0.95^(1/4)  # discount factor
params$gamma <- 0.57721566490153*0 # Euler's gamma constant
params$tol <- 1E-6 # stopping condition for the Bellman equation solution
params$nregoutcomes <- 80 # number of regulatory outcomes
params$ntransitions <- 3  # number of states transitions from omega to omega_tilde
################################################################################
# Create structures for fast computation of Bellman equation
params <- AddParams(params)
################################################################################
# Solve the Bellman equation
coeff_BGL <- c(2.872, -0.049, -0.077, -5.980, -0.065) # BGL
outBellman <- Bellman(coeff_BGL, params)
data_bellman <- read.csv(bellman_data_file)
data <- subset(data_bellman,naics_recode == 1)
data$index <- 1:nrow(data)
# Read analysis data for lilkelihood computations
panel_data <- read.csv(panel_data_file)
panel_data <- subset(panel_data,naics_recode == 1)
# Define the working parameters
params <- list()
params$data <- data
params$panel_data <- panel_data
params$N1 <- 161  # number of states per each omega1
params$n_omega1 <- length(unique((data$omega1))) # number of omega1 states
params$DAVgrid <- (0:19)/19*9.5  # DAV grid
params$n_DAV <- length(params$DAVgrid) # number of points in DAV grid
params$DAV_deprate <- 0.1  # DAV depreciation rate
params$beta <- 0.95^(1/4)  # discount factor
params$gamma <- 0.57721566490153*0 # Euler's gamma constant
params$tol <- 1E-6 # stopping condition for the Bellman equation solution
params$nregoutcomes <- 80 # number of regulatory outcomes
params$ntransitions <- 3  # number of states transitions from omega to omega_tilde
################################################################################
# Create structures for fast computation of Bellman equation
params <- AddParams(params)
Q
Q
Q
Q
Q
Q
# Set working directories
#current_dir <- "./IO_partII"
current_dir <- "G:/Mi unidad/Columbia_doc/My Classes/IO I/Assignment/final_version/IO_partII/"
setwd(current_dir)
outputs_dir <- paste0(current_dir,"Outputs")
bellman_data_file <- paste0(current_dir,"Data/data_for_bellman_computations.csv")
panel_data_file <- paste0(current_dir,"Data/analysis_data.csv")
source("Bellman_functions.R")
# Read Bellman data
data_bellman <- read.csv(bellman_data_file)
data <- subset(data_bellman,naics_recode == 1)
data$index <- 1:nrow(data)
# Read analysis data for lilkelihood computations
panel_data <- read.csv(panel_data_file)
panel_data <- subset(panel_data,naics_recode == 1)
# Define the working parameters
params <- list()
params$data <- data
params$panel_data <- panel_data
params$N1 <- 161  # number of states per each omega1
params$n_omega1 <- length(unique((data$omega1))) # number of omega1 states
params$DAVgrid <- (0:19)/19*9.5  # DAV grid
params$n_DAV <- length(params$DAVgrid) # number of points in DAV grid
params$DAV_deprate <- 0.1  # DAV depreciation rate
params$beta <- 0.95^(1/4)  # discount factor
params$gamma <- 0.57721566490153*0 # Euler's gamma constant
params$tol <- 1E-6 # stopping condition for the Bellman equation solution
params$nregoutcomes <- 80 # number of regulatory outcomes
params$ntransitions <- 3  # number of states transitions from omega to omega_tilde
################################################################################
# Create structures for fast computation of Bellman equation
params <- AddParams(params)
coeff_BGL <- c(2.872, -0.049, -0.077, -5.980, -0.065) # BGL
resultsBGL <- Bellman(coeff_BGL, params)
# Check resuts
ExpectedResult1 <- paste0(dir,"Data/value_part2_problem1_thetaBGL.csv")
CompareValues(resultsBGL,ExpectedResult1,params)
ExpectedResult2 <- paste0(dir,"Data/valuetilde_part2_problem1_thetaBGL.csv")
CompareVTilde(resultsBGL,ExpectedResult2,params)
# Check resuts
ExpectedResult1 <- paste0(current_dir,"Data/value_part2_problem1_thetaBGL.csv")
CompareValues(resultsBGL,ExpectedResult1,params)
ExpectedResult2 <- paste0(current_dir,"Data/valuetilde_part2_problem1_thetaBGL.csv")
CompareVTilde(resultsBGL,ExpectedResult2,params)
################################################################################
# Compute Loglikelihood
LogLikelihoodBGL <- LogLike(resultsBGL,params)
# Check resuts
ExpectedResult3 <- paste0(current_dir,"Data/likelihood_part2_problem2_thetaBGL.csv")
compTable<-CompareProb(LogLikelihoodBGL$lldata,ExpectedResult3)
coeff_1C <- c(2,-0.5,-0.5,-5,-0.1)
results1C <- Bellman(coeff_1C, params)
LogLike1C <- LogLike(results1C,params)
filename1C <- "Output/prob_DAV2_Q1C.csv"
Question1C(LogLike1C,filename1C)
################################################################################
# Prob 2.a: Report quasilikelihood
LogLike1C$ll
thetaML <- c(2.847, -1.084, -1.751, -3.890, 0.312)
DeltaTheta <- 1e-6
varML <- EstimateMLVariance(thetaML,params,DeltaTheta)
seML <- sqrt(diag(varML))
coeff_1C <- c(2,-0.5,-0.5,-5,-0.1)
results1C <- Bellman(coeff_1C, params)
LogLike1C <- LogLike(results1C,params)
filename1C <- "Output/prob_DAV2_Q1C.csv"
Question1C(LogLike1C,filename1C)
Question1C(LogLike1C,filename1C)
source('G:/Mi unidad/Columbia_doc/My Classes/IO I/Assignment/final_version/IO_partII/Bellman_functions.R')
Question1C(LogLike1C,filename1C)
Question1C(LogLike1C,filename1C)
Question1C(LogLike1C,filename1C)
Question1C(LogLike1C,filename1C)
Question1C(LogLike1C,filename1C)
source('G:/Mi unidad/Columbia_doc/My Classes/IO I/Assignment/final_version/IO_partII/Bellman_functions.R')
Question1C(results1C,filename1C)
